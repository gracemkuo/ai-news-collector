#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼·ç‰ˆAI News Collector - é•·æ‘˜è¦ç‰ˆæœ¬
"""

import os
import csv
import json
import hashlib
import requests
import feedparser
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import re
from dataclasses import dataclass
import logging
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import time
from urllib.parse import urljoin, urlparse
import xml.etree.ElementTree as ET

# è¨­å®šlogging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


@dataclass
class Article:
    """æ–‡ç« è³‡æ–™çµæ§‹"""
    title: str
    url: str
    summary: str
    content: str
    published_date: str
    source: str
    category: str
    relevance_score: float
    sentiment_score: float
    hash_id: str
    ai_summary: str = ""

class DataManager:
    """è³‡æ–™ç®¡ç†å™¨ - ä½¿ç”¨CSVå„²å­˜"""
    
    def __init__(self, csv_file: str = "ai_news_data.csv"):
        self.csv_file = csv_file
        self.fieldnames = [
            'hash_id', 'title', 'url', 'summary', 'content', 
            'published_date', 'source', 'category', 'relevance_score',
            'sentiment_score', 'ai_summary', 'collected_date'
        ]
        self._ensure_csv_exists()
    
    def _ensure_csv_exists(self):
        """ç¢ºä¿CSVæª”æ¡ˆå­˜åœ¨"""
        if not os.path.exists(self.csv_file):
            with open(self.csv_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=self.fieldnames)
                writer.writeheader()
    
    def article_exists(self, hash_id: str) -> bool:
        """æª¢æŸ¥æ–‡ç« æ˜¯å¦å·²å­˜åœ¨"""
        try:
            df = pd.read_csv(self.csv_file)
            return hash_id in df['hash_id'].values
        except (FileNotFoundError, pd.errors.EmptyDataError):
            return False
    
    def save_article(self, article: Article):
        """å„²å­˜æ–‡ç« """
        if self.article_exists(article.hash_id):
            logger.info(f"æ–‡ç« å·²å­˜åœ¨: {article.title[:50]}...")
            return
        
        with open(self.csv_file, 'a', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=self.fieldnames)
            writer.writerow({
                'hash_id': article.hash_id,
                'title': article.title,
                'url': article.url,
                'summary': article.summary,
                'content': article.content,
                'published_date': article.published_date,
                'source': article.source,
                'category': article.category,
                'relevance_score': article.relevance_score,
                'sentiment_score': article.sentiment_score,
                'ai_summary': article.ai_summary,
                'collected_date': datetime.now().isoformat()
            })
    
    def get_articles_by_date(self, date: str) -> List[Dict]:
        """æ ¹æ“šæ—¥æœŸå–å¾—æ–‡ç« """
        try:
            df = pd.read_csv(self.csv_file)
            # ğŸ‘‡ æ”¹å–„æ—¥æœŸè§£æ
            df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce', format='mixed')
            target_date = pd.to_datetime(date)
            
            # ç§»é™¤ç„¡æ³•è§£æçš„æ—¥æœŸ
            df = df.dropna(subset=['published_date'])
            
            filtered_df = df[df['published_date'].dt.date == target_date.date()]
            return filtered_df.to_dict('records')
        except (FileNotFoundError, pd.errors.EmptyDataError):
            return []
        except Exception as e:
            logger.error(f"å–å¾—æ–‡ç« æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []

class RSSCollector:
    """RSS Feedæ”¶é›†å™¨"""
    
    # å®Œæ•´RSSä¾†æºæ¸…å–® - ä½ æŒ‡å®šçš„ç‰ˆæœ¬
    RSS_FEEDS = {
        # ä¸»è¦ç§‘æŠ€åª’é«”
        'MIT Technology Review': 'https://www.technologyreview.com/feed/',
        'Nature AI': 'https://www.nature.com/subjects/machine-learning.rss',
        'IEEE Spectrum': 'https://spectrum.ieee.org/rss/fulltext',
        
        # AIå…¬å¸å®˜æ–¹éƒ¨è½æ ¼
        'OpenAI Blog': 'https://openai.com/blog/rss.xml',
        'Google AI Research': 'https://research.google/blog/rss/',
        'Anthropic Blog': 'https://www.anthropic.com/feed.xml',
        'Amazon AI Blog': 'https://aws.amazon.com/blogs/machine-learning/feed/',
        
        # å­¸è¡“æœŸåˆŠ
        'arXiv AI': 'http://export.arxiv.org/rss/cs.AI',
        'arXiv ML': 'http://export.arxiv.org/rss/cs.LG',
        'arXiv Bio': 'http://export.arxiv.org/rss/q-bio',
        
        # ç§‘æŠ€æ–°è
        'TechCrunch AI': 'https://techcrunch.com/category/artificial-intelligence/feed/',
        'VentureBeat AI': 'https://venturebeat.com/category/ai/feed/',
        
        # ç”Ÿç‰©ç§‘æŠ€
        'Nature Biotechnology': 'https://www.nature.com/nbt.rss',
        'BioPharma Dive': 'https://www.biopharmadive.com/feeds/news/',
        'GenomeWeb': 'https://www.genomeweb.com/section/rss/news?access_control=46',
        
        # é¡å¤–ä¾†æº
        'AI Business': 'https://aibusiness.com/rss.xml',
        'AI News': 'https://www.artificialintelligence-news.com/feed/rss/',
        'Towards Data Science': 'https://towardsdatascience.com/feed',
        'Machine Learning Mastery': 'https://machinelearningmastery.com/blog/feed/',
        'NVIDIA AI Blog': 'https://feeds.feedburner.com/nvidiablog',
    }
    
    def collect_rss_articles(self, max_articles_per_feed: int = 10, max_total_articles: int = 100) -> List[Article]:
        """æ”¶é›†RSSæ–‡ç« """
        articles = []
        
        for source_name, feed_url in self.RSS_FEEDS.items():
            if len(articles) >= max_total_articles:
                logger.info(f"å·²é”åˆ°æœ€å¤§æ–‡ç« æ•¸é™åˆ¶ ({max_total_articles})ï¼Œåœæ­¢æ”¶é›†")
                break
            try:
                logger.info(f"æ­£åœ¨æ”¶é›† {source_name} çš„RSS...")
                feed = feedparser.parse(feed_url)
                source_articles = 0
                for entry in feed.entries[:max_articles_per_feed]:
                    if source_articles >= max_articles_per_feed or len(articles) >= max_total_articles:
                        break
                    try:
                        # æå–æ–‡ç« è³‡è¨Š
                        title = entry.get('title', '').strip()
                        url = entry.get('link', '')
                        summary = entry.get('summary', entry.get('description', ''))
                        
                        # è™•ç†ç™¼ä½ˆæ—¥æœŸ
                        published_date = self._parse_date(entry)
                        
                        # ç”Ÿæˆå”¯ä¸€ID
                        hash_id = hashlib.md5(f"{title}{url}".encode()).hexdigest()
                        
                        # éæ¿¾AIç›¸é—œå…§å®¹
                        # æ›´åš´æ ¼çš„AIç›¸é—œå…§å®¹éæ¿¾
                        if self._is_ai_related(title, summary):
                            published_date = self._parse_date(entry)
                            hash_id = hashlib.md5(f"{title}{url}".encode()).hexdigest()
                            
                            article = Article(
                                title=title,
                                url=url,
                                summary=self._clean_text(summary),
                                content="",
                                published_date=published_date,
                                source=source_name,
                                category="AI",
                                relevance_score=0.0,
                                sentiment_score=0.0,
                                hash_id=hash_id
                            )
                            articles.append(article)
                            source_articles += 1
                    
                    except Exception as e:
                        logger.error(f"è™•ç†æ–‡ç« æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        continue
                
                # é¿å…è«‹æ±‚éæ–¼é »ç¹
                time.sleep(1)
                
            except Exception as e:
                logger.error(f"æ”¶é›† {source_name} RSSæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue
        
        logger.info(f"å…±æ”¶é›†åˆ° {len(articles)} ç¯‡æ–‡ç« ")
        return articles
    
    def _parse_date(self, entry) -> str:
        """è§£æç™¼ä½ˆæ—¥æœŸ"""
        date_fields = ['published', 'updated', 'pubDate']
        for field in date_fields:
            if hasattr(entry, field):
                try:
                    date_tuple = getattr(entry, f"{field}_parsed", None)
                    if date_tuple:
                        return datetime(*date_tuple[:6]).isoformat()
                except:
                    continue
        
        return datetime.now().isoformat()
    
    def _is_ai_related(self, title: str, summary: str) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç‚ºAIç›¸é—œå…§å®¹"""
        ai_keywords = [
            'artificial intelligence', 'ai', 'machine learning', 'ml', 'deep learning',
            'neural network', 'chatgpt', 'gpt', 'llm', 'large language model',
            'computer vision', 'natural language', 'nlp', 'robotics', 'automation',
            'biotech', 'biotechnology', 'bioinformatics', 'computational biology',
            'drug discovery', 'protein folding', 'genomics', 'crispr','DNA','RNA','protein'
        ]
        
        text = f"{title} {summary}".lower()
        return any(keyword in text for keyword in ai_keywords)
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡å­—å…§å®¹"""
        # ç§»é™¤HTMLæ¨™ç±¤
        text = re.sub(r'<[^>]+>', '', text)
        # ç§»é™¤å¤šé¤˜ç©ºç™½
        text = re.sub(r'\s+', ' ', text).strip()
        return text[:1000]

class ClaudeProcessor:
    """Claude APIè™•ç†å™¨ - å¢å¼·ç‰ˆé•·æ‘˜è¦"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.anthropic.com/v1/messages"
        self.max_retries = 3
        self.base_delay = 2
    
    def classify_and_summarize(self, article: Article) -> Article:
        """ä½¿ç”¨Claudeé€²è¡Œåˆ†é¡å’Œæ‘˜è¦ - ç”Ÿæˆæ›´é•·çš„æ‘˜è¦"""
        for attempt in range(self.max_retries):
            try:
                headers = {
                    "Content-Type": "application/json",
                    "x-api-key": self.api_key,
                    "anthropic-version": "2023-06-01"
                }
                
                    # ç°¡åŒ–promptä»¥æé«˜æˆåŠŸç‡
                prompt = f"""
                    è«‹åˆ†æä»¥ä¸‹æ–‡ç« ä¸¦æä¾›JSONæ ¼å¼å›ç­”ï¼š

                    æ¨™é¡Œ: {article.title[:200]}
                    æ‘˜è¦: {article.summary[:300]}
                    ä¾†æº: {article.source}

                    è«‹æä¾›JSONæ ¼å¼ï¼š
                    {{
                        "category": "AI Research/Industry News/Biotech/Otherä¸­é¸ä¸€",
                        "relevance_score": 0.85,
                        "sentiment_score": 0.0,
                        "ai_summary": "100-150å­—ç¹é«”ä¸­æ–‡æ‘˜è¦ï¼ŒåŒ…å«ä¸»è¦å…§å®¹å’ŒæŠ€è¡“æ„ç¾©"
                    }}
                    """
                
                data = {
                    "model": "claude-3-5-sonnet-20241022",  # ä½¿ç”¨æ›´å¼·çš„æ¨¡å‹ç”Ÿæˆæ›´å¥½çš„æ‘˜è¦
                    "max_tokens": 500,  # å¢åŠ tokenæ•¸é‡æ”¯æ´æ›´é•·æ‘˜è¦
                    "messages": [
                        {
                            "role": "user",
                            "content": [{"role": "user", "content": prompt}]
                        }
                    ]
                }
                
                response = requests.post(self.base_url, headers=headers, json=data,timeout=30)
                
                if response.status_code == 200:
                    result = response.json()
                    content = result['content'][0]['text']
                    
                    # è§£æJSON
                    try:
                            # å˜—è©¦å¾å…§å®¹ä¸­æå–JSON
                            json_match = re.search(r'\{.*\}', content, re.DOTALL)
                            if json_match:
                                parsed_result = json.loads(json_match.group())
                                
                                article.category = parsed_result.get('category', 'Other')
                                article.relevance_score = float(parsed_result.get('relevance_score', 0.5))
                                article.sentiment_score = float(parsed_result.get('sentiment_score', 0.0))
                                article.ai_summary = parsed_result.get('ai_summary', article.summary)
                                
                                logger.info(f"âœ… Claudeè™•ç†å®Œæˆ: {article.title[:30]}...")
                                return article
                            else:
                                raise json.JSONDecodeError("No JSON found", content, 0)
                                
                    except json.JSONDecodeError as e:
                            logger.warning(f"JSONè§£æå¤±æ•— (å˜—è©¦ {attempt + 1}/{self.max_retries}): {e}")
                            if attempt == self.max_retries - 1:
                                # æœ€å¾Œä¸€æ¬¡å˜—è©¦å¤±æ•—ï¼Œä½¿ç”¨é è¨­å€¼
                                article.category = "Other"
                                article.relevance_score = 0.5
                                article.sentiment_score = 0.0
                                article.ai_summary = article.summary[:150]
                                return article
                    
                elif response.status_code == 529:
                        wait_time = self.base_delay * (2 ** attempt)
                        logger.warning(f"APIç¹å¿™ (529)ï¼Œç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                        time.sleep(wait_time)
                        continue
                else:
                        logger.error(f"Claude APIéŒ¯èª¤: {response.status_code}")
                        break
                        
            except requests.exceptions.Timeout:
                    logger.warning(f"è«‹æ±‚è¶…æ™‚ (å˜—è©¦ {attempt + 1}/{self.max_retries})")
                    time.sleep(self.base_delay)
                    continue
            except Exception as e:
                    logger.error(f"Claudeè™•ç†éŒ¯èª¤: {e}")
                    break
        
        # æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—ï¼Œä½¿ç”¨é è¨­å€¼
        article.category = "Other"
        article.relevance_score = 0.5
        article.sentiment_score = 0.0
        article.ai_summary = article.summary[:150] if article.summary else "ç„¡æ³•ç”Ÿæˆæ‘˜è¦"
        return article
    
    # def _extract_summary_from_text(self, text: str) -> Optional[str]:
    #     """å¾Claudeå›æ‡‰æ–‡å­—ä¸­æå–æ‘˜è¦"""
    #     try:
    #         # å°‹æ‰¾å¯èƒ½çš„æ‘˜è¦å…§å®¹
    #         summary_patterns = [
    #             r'"ai_summary":\s*"([^"]+)"',
    #             r'æ‘˜è¦[ï¼š:]\s*(.+?)(?:\n|$)',
    #             r'ç¸½çµ[ï¼š:]\s*(.+?)(?:\n|$)'
    #         ]
            
    #         for pattern in summary_patterns:
    #             match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
    #             if match:
    #                 return match.group(1).strip()
            
    #         return None
    #     except:
    #         return None

class EmailSender:
    """éƒµä»¶ç™¼é€å™¨ - å¢å¼·ç‰ˆHTMLæ ¼å¼"""
    
    def __init__(self, smtp_server: str, smtp_port: int, email: str, password: str):
        self.smtp_server = smtp_server
        self.smtp_port = smtp_port
        self.email = email
        self.password = password
    
    def send_daily_report(self, articles: List[Dict], date: str):
        """ç™¼é€æ¯æ—¥å ±å‘Š"""
        if not articles:
            logger.info("ä»Šæ—¥ç„¡æ–°æ–‡ç« ï¼Œä¸ç™¼é€éƒµä»¶")
            return
        
        # ç”ŸæˆHTMLéƒµä»¶å…§å®¹
        html_content = self._generate_enhanced_html_report(articles, date)
        
        # å»ºç«‹éƒµä»¶
        msg = MIMEMultipart('alternative')
        msg['Subject'] = f'ğŸ¤– AIæ–°èæ—¥å ± - {date} ({len(articles)}ç¯‡ç²¾é¸)'
        msg['From'] = self.email
        msg['To'] = self.email
        
        html_part = MIMEText(html_content, 'html', 'utf-8')
        msg.attach(html_part)
        
        # ç™¼é€éƒµä»¶
        try:
            with smtplib.SMTP(self.smtp_server, self.smtp_port) as server:
                server.starttls()
                server.login(self.email, self.password)
                server.send_message(msg)
            
            logger.info(f"éƒµä»¶ç™¼é€æˆåŠŸ: {len(articles)} ç¯‡æ–‡ç« ")
            
        except Exception as e:
            logger.error(f"éƒµä»¶ç™¼é€å¤±æ•—: {e}")
    
    def _generate_enhanced_html_report(self, articles: List[Dict], date: str) -> str:
        """ç”Ÿæˆå¢å¼·ç‰ˆHTMLå ±å‘Š - åŒ…å«æ–‡ç« åŸå§‹ç™¼å¸ƒæ™‚é–“"""
        # æŒ‰ç›¸é—œæ€§åˆ†æ•¸æ’åº
        sorted_articles = sorted(articles, key=lambda x: x.get('relevance_score', 0), reverse=True)
        
        # åˆ†é¡çµ±è¨ˆ
        categories = {}
        sentiment_stats = {'positive': 0, 'neutral': 0, 'negative': 0}
        
        for article in sorted_articles:
            category = article.get('category', 'Other')
            categories[category] = categories.get(category, 0) + 1
            
            sentiment = article.get('sentiment_score', 0)
            if sentiment > 0.3:
                sentiment_stats['positive'] += 1
            elif sentiment < -0.3:
                sentiment_stats['negative'] += 1
            else:
                sentiment_stats['neutral'] += 1
        
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="utf-8">
            <title>AIæ–°èæ—¥å ± - {date}</title>
            <style>
                body {{ 
                    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif; 
                    margin: 0; 
                    padding: 20px;
                    line-height: 1.6; 
                    color: #333;
                    background-color: #f5f5f5;
                }}
                .container {{
                    max-width: 800px;
                    margin: 0 auto;
                    background-color: white;
                    border-radius: 12px;
                    box-shadow: 0 4px 6px rgba(0,0,0,0.1);
                    overflow: hidden;
                }}
                .header {{ 
                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                    color: white;
                    padding: 30px 20px;
                    text-align: center;
                }}
                .header h1 {{
                    margin: 0;
                    font-size: 28px;
                    font-weight: 600;
                }}
                .header p {{
                    margin: 10px 0 0 0;
                    opacity: 0.9;
                    font-size: 16px;
                }}
                .stats {{ 
                    background: linear-gradient(135deg, #e8f4fd 0%, #f0f8ff 100%);
                    padding: 20px;
                    margin: 0;
                    border-bottom: 1px solid #e1e8ed;
                }}
                .stats h3 {{
                    color: #1a73e8;
                    margin-top: 0;
                    font-size: 18px;
                }}
                .stats-grid {{
                    display: grid;
                    grid-template-columns: repeat(auto-fit, minmax(120px, 1fr));
                    gap: 15px;
                    margin-top: 15px;
                }}
                .stat-item {{
                    text-align: center;
                    padding: 10px;
                    background: white;
                    border-radius: 8px;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.05);
                }}
                .stat-number {{
                    display: block;
                    font-size: 24px;
                    font-weight: bold;
                    color: #1a73e8;
                }}
                .stat-label {{
                    font-size: 12px;
                    color: #666;
                    margin-top: 5px;
                }}
                .content {{
                    padding: 20px;
                }}
                .section-title {{
                    color: #1a73e8;
                    font-size: 20px;
                    font-weight: 600;
                    margin: 30px 0 15px 0;
                    padding-bottom: 8px;
                    border-bottom: 2px solid #e8f4fd;
                }}
                .article {{ 
                    margin: 20px 0; 
                    padding: 20px;
                    border-left: 4px solid #1a73e8; 
                    background: linear-gradient(135deg, #fafbfc 0%, #f8f9fa 100%);
                    border-radius: 0 8px 8px 0;
                    transition: transform 0.2s ease;
                }}
                .article:hover {{
                    transform: translateX(5px);
                    box-shadow: 0 4px 12px rgba(0,0,0,0.1);
                }}
                .article h3 {{ 
                    color: #1a73e8; 
                    margin-top: 0; 
                    font-size: 18px;
                    line-height: 1.4;
                }}
                .article h3 a {{
                    color: #1a73e8;
                    text-decoration: none;
                }}
                .article h3 a:hover {{
                    text-decoration: underline;
                }}
                .meta {{ 
                    color: #666; 
                    font-size: 14px; 
                    margin: 8px 0;
                    display: flex;
                    flex-wrap: wrap;
                    gap: 15px;
                    align-items: center;
                }}
                .meta-item {{
                    display: flex;
                    align-items: center;
                    gap: 5px;
                }}
                .summary {{ 
                    margin: 15px 0;
                    font-size: 15px;
                    line-height: 1.6;
                    color: #444;
                    text-align: justify;
                }}
                .score {{ font-weight: bold; }}
                .high-score {{ color: #34a853; }}
                .medium-score {{ color: #ea4335; }}
                .low-score {{ color: #fbbc05; }}
                .category-tag {{
                    background: #e8f4fd;
                    color: #1a73e8;
                    padding: 3px 8px;
                    border-radius: 12px;
                    font-size: 12px;
                    font-weight: 500;
                }}
                .url-link {{
                    display: inline-block;
                    margin-top: 10px;
                    padding: 8px 16px;
                    background: #1a73e8;
                    color: white;
                    text-decoration: none;
                    border-radius: 6px;
                    font-size: 14px;
                    transition: background 0.2s ease;
                }}
                .url-link:hover {{
                    background: #1557b0;
                    color: white;
                    text-decoration: none;
                }}
                .footer {{
                    text-align: center;
                    padding: 20px;
                    background: #f8f9fa;
                    color: #666;
                    font-size: 14px;
                    border-top: 1px solid #e1e8ed;
                }}
                .divider {{
                    height: 1px;
                    background: linear-gradient(90deg, transparent, #e1e8ed, transparent);
                    margin: 25px 0;
                }}
                @media (max-width: 600px) {{
                    .stats-grid {{
                        grid-template-columns: repeat(2, 1fr);
                    }}
                    .meta {{
                        flex-direction: column;
                        align-items: flex-start;
                        gap: 8px;
                    }}
                }}
            </style>
        </head>
        <body>
            <div class="container">
                <div class="header">
                    <h1>ğŸ¤– AIæ–°èæ—¥å ±</h1>
                    <p>æ—¥æœŸ: {date} | å…±æ”¶é›† {len(sorted_articles)} ç¯‡ç²¾é¸æ–‡ç« </p>
                </div>
                
                <div class="stats">
                    <h3>ğŸ“Š ä»Šæ—¥æ‘˜è¦çµ±è¨ˆ</h3>
                    <div class="stats-grid">
                        <div class="stat-item">
                            <span class="stat-number">{len(sorted_articles)}</span>
                            <div class="stat-label">ç¸½æ–‡ç« æ•¸</div>
                        </div>
                        <div class="stat-item">
                            <span class="stat-number">{sum(1 for a in sorted_articles if a.get('relevance_score', 0) > 0.8)}</span>
                            <div class="stat-label">é«˜ç›¸é—œæ€§</div>
                        </div>
                        <div class="stat-item">
                            <span class="stat-number">{sentiment_stats['positive']}</span>
                            <div class="stat-label">æ­£é¢æ–°è</div>
                        </div>
                        <div class="stat-item">
                            <span class="stat-number">{sentiment_stats['neutral']}</span>
                            <div class="stat-label">ä¸­æ€§å ±å°</div>
                        </div>
                        <div class="stat-item">
                            <span class="stat-number">{sentiment_stats['negative']}</span>
                            <div class="stat-label">è² é¢æ¶ˆæ¯</div>
                        </div>
                        <div class="stat-item">
                            <span class="stat-number">{round(sum(a.get('relevance_score', 0) for a in sorted_articles) / len(sorted_articles), 2) if sorted_articles else 0}</span>
                            <div class="stat-label">å¹³å‡ç›¸é—œæ€§</div>
                        </div>
                    </div>
                    
                    <h4 style="margin-top: 20px; margin-bottom: 10px; color: #1a73e8;">ğŸ“ˆ åˆ†é¡åˆ†å¸ƒ</h4>
                    <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(140px, 1fr)); gap: 10px;">
        """
        
        # åŠ å…¥åˆ†é¡çµ±è¨ˆ
        category_icons = {
            'AI Research': 'ğŸ”¬',
            'Industry News': 'ğŸ¢', 
            'Biotech': 'ğŸ§¬',
            'Robotics': 'ğŸ¤–',
            'Other': 'ğŸ“±'
        }
        
        for category, count in categories.items():
            icon = category_icons.get(category, 'ğŸ“„')
            html += f'<div>{icon} <strong>{category}:</strong> {count}ç¯‡</div>'
        
        html += """
                    </div>
                </div>

                <div class="content">
                    <h2 class="section-title">ğŸŒŸ ä»Šæ—¥ç²¾é¸æ–‡ç« </h2>
        """
        
        # åŠ å…¥æ–‡ç« å…§å®¹
        for i, article in enumerate(sorted_articles[:20], 1):  # é™åˆ¶æœ€å¤š20ç¯‡
            relevance = article.get('relevance_score', 0)
            sentiment = article.get('sentiment_score', 0)
            
            # ç›¸é—œæ€§é¡è‰²
            if relevance >= 0.8:
                score_class = "high-score"
            elif relevance >= 0.6:
                score_class = "medium-score"
            else:
                score_class = "low-score"
            
            # æƒ…æ„Ÿç¬¦è™Ÿ
            if sentiment > 0.3:
                sentiment_icon = "ğŸ˜Š"
            elif sentiment < -0.3:
                sentiment_icon = "ğŸ˜Ÿ"
            else:
                sentiment_icon = "ğŸ˜"
            
            # æ–‡ç« URL - ç¢ºä¿æœ‰æ•ˆé€£çµ
            article_url = article.get('url', '#')
            if not article_url.startswith('http'):
                article_url = '#'
            
            # è™•ç†ç™¼å¸ƒæ—¥æœŸé¡¯ç¤º
            published_date = article.get('published_date', '')
            date_display = ""
            if published_date:
                try:
                    from datetime import datetime
                    # è§£æISOæ ¼å¼æ—¥æœŸ
                    if 'T' in published_date:
                        pub_datetime = datetime.fromisoformat(published_date.replace('Z', '+00:00').replace('+00:00', ''))
                    else:
                        pub_datetime = datetime.fromisoformat(published_date)
                    
                    # è¨ˆç®—å¤©æ•¸å·®
                    now = datetime.now()
                    days_diff = (now - pub_datetime).days
                    
                    if days_diff == 0:
                        date_display = f"ğŸ“… ä»Šå¤© {pub_datetime.strftime('%H:%M')}"
                    elif days_diff == 1:
                        date_display = f"ğŸ“… æ˜¨å¤© {pub_datetime.strftime('%H:%M')}"
                    elif days_diff <= 7:
                        date_display = f"ğŸ“… {days_diff}å¤©å‰ ({pub_datetime.strftime('%m-%d %H:%M')})"
                    else:
                        date_display = f"ğŸ“… {pub_datetime.strftime('%Y-%m-%d %H:%M')}"
                except:
                    date_display = f"ğŸ“… {published_date[:10]}"
            
            html += f"""
            <div class="article">
                <h3>{i}. <a href="{article_url}" target="_blank">{article.get('title', 'ç„¡æ¨™é¡Œ')}</a></h3>
                <div class="meta">
                    <div class="meta-item">ğŸ“° ä¾†æº: <strong>{article.get('source', 'æœªçŸ¥')}</strong></div>
                    <div class="meta-item">ğŸ·ï¸ <span class="category-tag">{article.get('category', 'Other')}</span></div>
                    <div class="meta-item"><span class="score {score_class}">â­ ç›¸é—œæ€§: {relevance:.2f}</span></div>
                    <div class="meta-item">{sentiment_icon} æƒ…æ„Ÿ: {sentiment:.2f}</div>
                </div>
                <div class="publish-date">{date_display}</div>
                <div class="summary">
                    <strong>AIæ‘˜è¦:</strong> {article.get('ai_summary', article.get('summary', 'ç„¡æ‘˜è¦'))}
                </div>
                <a href="{article_url}" target="_blank" class="url-link">ğŸ“– é–±è®€åŸæ–‡</a>
            </div>
            """
        
        html += """
                </div>

                <div class="footer">
                    <p><strong>ğŸ“ˆ æ•¸æ“šæ´å¯Ÿ</strong></p>
                    <p>æœ¬æœŸAIæ–°èæ¶µè“‹äº†æœ€æ–°çš„æŠ€è¡“çªç ´ã€ç”¢æ¥­å‹•æ…‹å’Œç ”ç©¶é€²å±•</p>
                    <p>æ‰€æœ‰æ–‡ç« å‡é™„ä¸ŠåŸæ–‡é€£çµï¼Œé»æ“Šå³å¯æ·±å…¥é–±è®€</p>
                    <hr style="margin: 20px 0; border: none; height: 1px; background: #e1e8ed;">
                    <p style="margin: 0;">
                        æ­¤å ±å‘Šç”±AIæ–°èæ”¶é›†ç³»çµ±è‡ªå‹•ç”Ÿæˆ | Powered by Claude AI<br>
                        <small>è³‡æ–™ä¾†æºæ¶µè“‹23å€‹æ¬Šå¨AI/ç§‘æŠ€åª’é«” | æ¯æ—¥å°åŒ—æ™‚é–“8:00æ›´æ–°</small>
                    </p>
                </div>
            </div>
        </body>
        </html>
        """
        
        return html

def main():
    """ä¸»ç¨‹å¼"""
    start_time = time.time()
    logger.info("é–‹å§‹åŸ·è¡ŒAIæ–°èæ”¶é›†ç¨‹åº...")
    
    # å¾ç’°å¢ƒè®Šæ•¸å–å¾—è¨­å®š
    claude_api_key = os.environ.get('CLAUDE_API_KEY')
    gmail_user = os.environ.get('GMAIL_USER')
    gmail_password = os.environ.get('GMAIL_APP_PASSWORD')
    
    if not all([claude_api_key, gmail_user, gmail_password]):
        logger.error("ç¼ºå°‘å¿…è¦çš„ç’°å¢ƒè®Šæ•¸")
        return
    
    # åˆå§‹åŒ–çµ„ä»¶
    data_manager = DataManager()
    rss_collector = RSSCollector()
    claude_processor = ClaudeProcessor(claude_api_key)
    email_sender = EmailSender('smtp.gmail.com', 587, gmail_user, gmail_password)
    
    # æ”¶é›†æ–‡ç« 
    logger.info("æ­£åœ¨æ”¶é›†RSSæ–‡ç« ...")
    articles = rss_collector.collect_rss_articles()
    max_process_time = 30 * 60  
    
    # è™•ç†æ–°æ–‡ç« 
    new_articles = []
    processed_count = 0
    
    for article in articles:
        if time.time() - start_time > max_process_time:
            logger.warning("é”åˆ°æ™‚é–“é™åˆ¶ï¼Œåœæ­¢è™•ç†æ–°æ–‡ç« ")
            break
        if not data_manager.article_exists(article.hash_id):
            # ä½¿ç”¨Claudeè™•ç† - ç”Ÿæˆé•·æ‘˜è¦
            logger.info(f"æ­£åœ¨è™•ç†æ–‡ç« : {article.title[:50]}...")
            processed_article = claude_processor.classify_and_summarize(article)
            data_manager.save_article(processed_article)
            new_articles.append(processed_article)
            processed_count += 1
            
            # æ¯è™•ç†5ç¯‡æ–‡ç« æš«åœä¸€ä¸‹ï¼Œé¿å…API rate limit
            if processed_count % 3 == 0:
                    logger.info(f"å·²è™•ç† {processed_count} ç¯‡æ–‡ç« ï¼Œæš«åœ3ç§’...")
                    time.sleep(3)
    
    logger.info(f"è™•ç†äº† {len(new_articles)} ç¯‡æ–°æ–‡ç« ")
    
    # æº–å‚™ä»Šæ—¥å ±å‘Š
    today = datetime.now().strftime('%Y-%m-%d')
    today_articles = data_manager.get_articles_by_date(today)
    
    # å¦‚æœä»Šæ—¥æ²’æœ‰æ–°æ–‡ç« ï¼Œå–å¾—æœ€è¿‘çš„æ–‡ç« 
    if not today_articles:
        try:
            df = pd.read_csv(data_manager.csv_file)
            df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')
            recent_df = df.dropna(subset=['published_date']).tail(20)  # æœ€è¿‘20ç¯‡
            today_articles = recent_df.to_dict('records')
        except:
            today_articles = []
    
    # ç™¼é€éƒµä»¶
    if today_articles:
        email_sender.send_daily_report(today_articles, today)
    else:
        logger.info("æ²’æœ‰æ–‡ç« å¯ç™¼é€")
    total_time = time.time() - start_time
    logger.info(f"AIæ–°èæ”¶é›†ç¨‹åºåŸ·è¡Œå®Œæˆ! ç¸½è€—æ™‚: {total_time:.1f}ç§’")

if __name__ == "__main__":
    main()
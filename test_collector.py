#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¸¬è©¦ç‰ˆAI News Collector - ç”¨æ–¼æœ¬åœ°æ¸¬è©¦
"""

import os
import csv
import json
import hashlib
import requests
import feedparser
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import re
from dataclasses import dataclass
import logging
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import time

# è¨­å®šlogging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class Article:
    """æ–‡ç« è³‡æ–™çµæ§‹"""
    title: str
    url: str
    summary: str
    content: str
    published_date: str
    source: str
    category: str
    relevance_score: float
    sentiment_score: float
    hash_id: str
    ai_summary: str = ""

class TestDataManager:
    """æ¸¬è©¦ç”¨è³‡æ–™ç®¡ç†å™¨"""
    
    def __init__(self, csv_file: str = "test_ai_news_data.csv"):
        self.csv_file = csv_file
        self.fieldnames = [
            'hash_id', 'title', 'url', 'summary', 'content', 
            'published_date', 'source', 'category', 'relevance_score',
            'sentiment_score', 'ai_summary', 'collected_date'
        ]
        self._ensure_csv_exists()
    
    def _ensure_csv_exists(self):
        """ç¢ºä¿CSVæª”æ¡ˆå­˜åœ¨"""
        if not os.path.exists(self.csv_file):
            with open(self.csv_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=self.fieldnames)
                writer.writeheader()
            logger.info(f"å»ºç«‹æ–°çš„æ¸¬è©¦è³‡æ–™æª”æ¡ˆ: {self.csv_file}")
    
    def article_exists(self, hash_id: str) -> bool:
        """æª¢æŸ¥æ–‡ç« æ˜¯å¦å·²å­˜åœ¨"""
        try:
            df = pd.read_csv(self.csv_file)
            return hash_id in df['hash_id'].values
        except (FileNotFoundError, pd.errors.EmptyDataError):
            return False
    
    def save_article(self, article: Article):
        """å„²å­˜æ–‡ç« """
        if self.article_exists(article.hash_id):
            logger.info(f"æ–‡ç« å·²å­˜åœ¨: {article.title[:50]}...")
            return
        
        with open(self.csv_file, 'a', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=self.fieldnames)
            writer.writerow({
                'hash_id': article.hash_id,
                'title': article.title,
                'url': article.url,
                'summary': article.summary,
                'content': article.content,
                'published_date': article.published_date,
                'source': article.source,
                'category': article.category,
                'relevance_score': article.relevance_score,
                'sentiment_score': article.sentiment_score,
                'ai_summary': article.ai_summary,
                'collected_date': datetime.now().isoformat()
            })
        logger.info(f"âœ… å„²å­˜æ–°æ–‡ç« : {article.title[:50]}...")

class TestRSSCollector:
    """æ¸¬è©¦ç”¨RSSæ”¶é›†å™¨ - åªå–å°‘é‡æ–‡ç« """
    
    # ç²¾é¸æ¸¬è©¦ç”¨RSSä¾†æº
# åœ¨ test_collector.py ä¸­ï¼Œæ›¿æ› TestRSSCollector çš„ RSS_FEEDS
    RSS_FEEDS = {
    # ä¸»è¦ç§‘æŠ€åª’é«”
    'MIT Technology Review': 'https://www.technologyreview.com/feed/',
    'Nature AI': 'https://www.nature.com/subjects/machine-learning.rss',
    'IEEE Spectrum': 'https://spectrum.ieee.org/rss/fulltext',
    'TechCrunch': 'https://feeds.feedburner.com/TechCrunch/',    
    # AIå…¬å¸å®˜æ–¹éƒ¨è½æ ¼
    'OpenAI Blog': 'https://openai.com/blog/rss.xml',
    'Google AI Research': 'https://research.google/blog/rss/',
    'Meta AI Blog': 'https://research.facebook.com/feed/',
    'Anthropic Blog': 'https://www.anthropic.com/feed.xml',
    'Amazon AI Blog': 'https://aws.amazon.com/blogs/machine-learning/feed/',
    'Microsoft AI Blog': 'https://blogs.microsoft.com/ai/feed/',
    
    # å­¸è¡“æœŸåˆŠ
    'arXiv AI': 'http://export.arxiv.org/rss/cs.AI',
    'arXiv ML': 'http://export.arxiv.org/rss/cs.LG',
    'arXiv Bio': 'http://export.arxiv.org/rss/q-bio',
    
    # ç§‘æŠ€æ–°è
    'TechCrunch AI': 'https://techcrunch.com/category/artificial-intelligence/feed/',
    'The Verge AI': 'https://www.theverge.com/rss/ai-artificial-intelligence/index.xml',
    'Wired AI': 'https://www.wired.com/feed/tag/ai/latest/rss',
    'VentureBeat AI': 'https://venturebeat.com/category/ai/feed/',
    
    # ç”Ÿç‰©ç§‘æŠ€
    'Nature Biotechnology': 'https://www.nature.com/nbt.rss',
    'BioPharma Dive': 'https://www.biopharmadive.com/feeds/news/',
    'GenomeWeb': 'https://www.genomeweb.com/section/rss/news?access_control=46',
    
    # é¡å¤–ä¾†æº
    'AI Business': 'https://aibusiness.com/rss.xml',
    'AI News': 'https://www.artificialintelligence-news.com/feed/rss/',
    'Towards Data Science': 'https://towardsdatascience.com/feed',
    'Machine Learning Mastery': 'https://machinelearningmastery.com/blog/feed/',
    'BAIR Blog': 'https://bair.berkeley.edu/blog/feed.xml',
    'NVIDIA AI Blog': 'https://feeds.feedburner.com/nvidiablog',
}
    
    def collect_rss_articles(self, max_articles_per_feed: int = 3) -> List[Article]:
        """æ”¶é›†RSSæ–‡ç«  - æ¸¬è©¦ç‰ˆæœ¬"""
        articles = []
        
        for source_name, feed_url in self.RSS_FEEDS.items():
            try:
                logger.info(f"ğŸ“¡ æ­£åœ¨æ¸¬è©¦æ”¶é›† {source_name}...")
                feed = feedparser.parse(feed_url)
                
                if not feed.entries:
                    logger.warning(f"âš ï¸  {source_name} æ²’æœ‰æ–‡ç« æˆ–ç„¡æ³•å­˜å–")
                    continue
                
                for entry in feed.entries[:max_articles_per_feed]:
                    try:
                        title = entry.get('title', '').strip()
                        url = entry.get('link', '')
                        summary = entry.get('summary', entry.get('description', ''))
                        
                        if not title or not url:
                            continue
                        
                        # è™•ç†ç™¼ä½ˆæ—¥æœŸ
                        published_date = self._parse_date(entry)
                        
                        # ç”Ÿæˆå”¯ä¸€ID
                        hash_id = hashlib.md5(f"{title}{url}".encode()).hexdigest()
                        
                        # ç°¡å–®çš„AIç›¸é—œéæ¿¾
                        if self._is_ai_related(title, summary):
                            article = Article(
                                title=title,
                                url=url,
                                summary=self._clean_text(summary),
                                content="",
                                published_date=published_date,
                                source=source_name,
                                category="AI",
                                relevance_score=0.0,
                                sentiment_score=0.0,
                                hash_id=hash_id
                            )
                            articles.append(article)
                            logger.info(f"   ğŸ“„ æ‰¾åˆ°AIç›¸é—œæ–‡ç« : {title[:50]}...")
                    
                    except Exception as e:
                        logger.error(f"è™•ç† {source_name} æ–‡ç« æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        continue
                
                # é¿å…è«‹æ±‚éæ–¼é »ç¹
                time.sleep(2)
                
            except Exception as e:
                logger.error(f"æ”¶é›† {source_name} RSSæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue
        
        logger.info(f"ğŸ¯ ç¸½å…±æ”¶é›†åˆ° {len(articles)} ç¯‡AIç›¸é—œæ–‡ç« ")
        return articles
    
    def _parse_date(self, entry) -> str:
        """è§£æç™¼ä½ˆæ—¥æœŸ"""
        date_fields = ['published', 'updated', 'pubDate']
        for field in date_fields:
            if hasattr(entry, field + '_parsed'):
                try:
                    date_tuple = getattr(entry, f"{field}_parsed", None)
                    if date_tuple:
                        return datetime(*date_tuple[:6]).isoformat()
                except:
                    continue
        
        return datetime.now().isoformat()
    
    def _is_ai_related(self, title: str, summary: str) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç‚ºAIç›¸é—œå…§å®¹"""
        ai_keywords = [
            'artificial intelligence', 'ai', 'machine learning', 'ml', 'deep learning',
            'neural network', 'chatgpt', 'gpt', 'llm', 'large language model',
            'computer vision', 'natural language', 'nlp', 'robotics', 'automation',
            'biotech', 'biotechnology', 'bioinformatics', 'computational biology'
        ]
        
        text = f"{title} {summary}".lower()
        return any(keyword in text for keyword in ai_keywords)
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡å­—å…§å®¹"""
        text = re.sub(r'<[^>]+>', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

class TestClaudeProcessor:
    """æ¸¬è©¦ç”¨Claudeè™•ç†å™¨"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.anthropic.com/v1/messages"
    
    def classify_and_summarize(self, article: Article) -> Article:
        """æ¸¬è©¦Claudeåˆ†é¡å’Œæ‘˜è¦åŠŸèƒ½"""
        try:
            logger.info(f"ğŸ¤– æ­£åœ¨ç”¨Claudeè™•ç†: {article.title[:30]}...")
            
            headers = {
                "Content-Type": "application/json",
                "x-api-key": self.api_key,
                "anthropic-version": "2023-06-01"
            }
            
            prompt = f"""
            è«‹åˆ†æä»¥ä¸‹AIç›¸é—œæ–‡ç« ä¸¦æä¾›JSONæ ¼å¼å›ç­”ï¼š
            
            æ–‡ç« æ¨™é¡Œ: {article.title}
            æ–‡ç« æ‘˜è¦: {article.summary[:500]}
            
            è«‹å›ç­”ï¼š
            1. å…§å®¹åˆ†é¡ (AI Research, Industry News, Biotech, Robotics, Other)
            2. ç›¸é—œæ€§è©•åˆ† (0-1, 1ç‚ºæœ€ç›¸é—œAIä¸»é¡Œ)
            3. æƒ…æ„Ÿåˆ†æè©•åˆ† (-1åˆ°1, -1è² é¢, 0ä¸­æ€§, 1æ­£é¢)
            4. ç¹é«”ä¸­æ–‡æ‘˜è¦ (80å­—ä»¥å…§)
            
            å›ç­”æ ¼å¼ï¼š
            {{
                "category": "åˆ†é¡",
                "relevance_score": 0.8,
                "sentiment_score": 0.2,
                "ai_summary": "ç¹é«”ä¸­æ–‡æ‘˜è¦"
            }}
            """
            
            data = {
                "model": "claude-3-haiku-20240307",  # ä½¿ç”¨è¼ƒä¾¿å®œçš„æ¨¡å‹æ¸¬è©¦
                "max_tokens": 500,
                "messages": [{"role": "user", "content": prompt}]
            }
            
            response = requests.post(self.base_url, headers=headers, json=data)
            
            if response.status_code == 200:
                result = response.json()
                content = result['content'][0]['text']
                
                try:
                    # è§£æJSONå›æ‡‰
                    parsed_result = json.loads(content)
                    article.category = parsed_result.get('category', 'Other')
                    article.relevance_score = float(parsed_result.get('relevance_score', 0.5))
                    article.sentiment_score = float(parsed_result.get('sentiment_score', 0.0))
                    article.ai_summary = parsed_result.get('ai_summary', article.summary)
                    
                    logger.info(f"   âœ… Claudeè™•ç†å®Œæˆ - åˆ†é¡: {article.category}, ç›¸é—œæ€§: {article.relevance_score:.2f}")
                    
                except json.JSONDecodeError:
                    logger.error(f"   âŒ ç„¡æ³•è§£æClaudeå›æ‡‰: {content}")
                    # ä½¿ç”¨é è¨­å€¼
                    article.category = "Other"
                    article.relevance_score = 0.5
                    article.sentiment_score = 0.0
                    article.ai_summary = article.summary
            else:
                logger.error(f"   âŒ Claude APIéŒ¯èª¤: {response.status_code} - {response.text}")
                # ä½¿ç”¨é è¨­å€¼
                article.category = "Other"
                article.relevance_score = 0.5
                article.sentiment_score = 0.0
                article.ai_summary = article.summary
            
            # é¿å…API rate limit
            time.sleep(3)
            
        except Exception as e:
            logger.error(f"Claudeè™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            # ä½¿ç”¨é è¨­å€¼
            article.category = "Other"
            article.relevance_score = 0.5
            article.sentiment_score = 0.0
            article.ai_summary = article.summary
        
        return article

def test_rss_only():
    """åªæ¸¬è©¦RSSæ”¶é›†åŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦RSSæ”¶é›†åŠŸèƒ½...")
    
    collector = TestRSSCollector()
    articles = collector.collect_rss_articles(max_articles_per_feed=2)
    
    print(f"\nğŸ“Š æ”¶é›†çµæœ:")
    print(f"ç¸½æ–‡ç« æ•¸: {len(articles)}")
    
    for i, article in enumerate(articles[:5], 1):
        print(f"\n{i}. {article.title}")
        print(f"   ä¾†æº: {article.source}")
        print(f"   æ‘˜è¦: {article.summary[:100]}...")
        print(f"   é€£çµ: {article.url}")

def test_claude_api():
    """æ¸¬è©¦Claude API"""
    api_key = os.environ.get('CLAUDE_API_KEY')
    if not api_key:
        print("âŒ è«‹è¨­å®š CLAUDE_API_KEY ç’°å¢ƒè®Šæ•¸")
        return
    
    print("ğŸ§ª æ¸¬è©¦Claude API...")
    
    processor = TestClaudeProcessor(api_key)
    
    # å»ºç«‹æ¸¬è©¦æ–‡ç« 
    test_article = Article(
        title="OpenAI Launches GPT-4 Turbo with Enhanced Capabilities",
        url="https://example.com/test",
        summary="OpenAI announced the release of GPT-4 Turbo, featuring improved reasoning capabilities and reduced costs for developers.",
        content="",
        published_date=datetime.now().isoformat(),
        source="Test Source",
        category="",
        relevance_score=0.0,
        sentiment_score=0.0,
        hash_id="test123"
    )
    
    processed_article = processor.classify_and_summarize(test_article)
    
    print(f"\nğŸ“Š Claudeè™•ç†çµæœ:")
    print(f"æ¨™é¡Œ: {processed_article.title}")
    print(f"åˆ†é¡: {processed_article.category}")
    print(f"ç›¸é—œæ€§: {processed_article.relevance_score}")
    print(f"æƒ…æ„Ÿåˆ†æ: {processed_article.sentiment_score}")
    print(f"AIæ‘˜è¦: {processed_article.ai_summary}")

def test_full_pipeline():
    """æ¸¬è©¦å®Œæ•´æµç¨‹"""
    print("ğŸ§ª æ¸¬è©¦å®Œæ•´AIæ–°èæ”¶é›†æµç¨‹...")
    
    # æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
    claude_api_key = os.environ.get('CLAUDE_API_KEY')
    if not claude_api_key:
        print("âŒ è«‹è¨­å®š CLAUDE_API_KEY ç’°å¢ƒè®Šæ•¸")
        return
    
    # åˆå§‹åŒ–çµ„ä»¶
    data_manager = TestDataManager()
    rss_collector = TestRSSCollector()
    claude_processor = TestClaudeProcessor(claude_api_key)
    
    # æ”¶é›†æ–‡ç« 
    print("\nğŸ“¡ æ­¥é©Ÿ1: æ”¶é›†RSSæ–‡ç« ...")
    articles = rss_collector.collect_rss_articles(max_articles_per_feed=2)
    
    # è™•ç†æ–‡ç« 
    print(f"\nğŸ¤– æ­¥é©Ÿ2: ç”¨Claudeè™•ç† {len(articles)} ç¯‡æ–‡ç« ...")
    new_articles = []
    
    for i, article in enumerate(articles, 1):
        if not data_manager.article_exists(article.hash_id):
            print(f"   è™•ç† {i}/{len(articles)}: {article.title[:40]}...")
            processed_article = claude_processor.classify_and_summarize(article)
            data_manager.save_article(processed_article)
            new_articles.append(processed_article)
        else:
            print(f"   è·³éå·²å­˜åœ¨æ–‡ç« : {article.title[:40]}...")
    
    print(f"\nâœ… å®Œæˆï¼è™•ç†äº† {len(new_articles)} ç¯‡æ–°æ–‡ç« ")
    
    # é¡¯ç¤ºçµæœ
    if new_articles:
        print(f"\nğŸ“Š è™•ç†çµæœæ‘˜è¦:")
        categories = {}
        total_relevance = 0
        
        for article in new_articles:
            categories[article.category] = categories.get(article.category, 0) + 1
            total_relevance += article.relevance_score
        
        print(f"åˆ†é¡çµ±è¨ˆ: {categories}")
        print(f"å¹³å‡ç›¸é—œæ€§: {total_relevance/len(new_articles):.2f}")
        
        print(f"\nğŸ“° æœ€ç›¸é—œçš„æ–‡ç« :")
        sorted_articles = sorted(new_articles, key=lambda x: x.relevance_score, reverse=True)
        for article in sorted_articles[:3]:
            print(f"â€¢ {article.title[:60]}...")
            print(f"  ç›¸é—œæ€§: {article.relevance_score:.2f} | {article.ai_summary[:80]}...")

def main():
    print("ğŸ§ª AI News Collector æ¸¬è©¦å·¥å…·")
    print("=" * 50)
    
    while True:
        print("\né¸æ“‡æ¸¬è©¦é …ç›®:")
        print("1. åªæ¸¬è©¦RSSæ”¶é›†")
        print("2. åªæ¸¬è©¦Claude API")
        print("3. æ¸¬è©¦å®Œæ•´æµç¨‹")
        print("4. æŸ¥çœ‹æ¸¬è©¦è³‡æ–™")
        print("5. é€€å‡º")
        
        choice = input("\nè«‹é¸æ“‡ (1-5): ").strip()
        
        if choice == '1':
            test_rss_only()
        elif choice == '2':
            test_claude_api()
        elif choice == '3':
            test_full_pipeline()
        elif choice == '4':
            os.system("python manual_trigger.py stats")
        elif choice == '5':
            print("ğŸ‘‹ æ¸¬è©¦çµæŸ")
            break
        else:
            print("âŒ ç„¡æ•ˆé¸æ“‡ï¼Œè«‹é‡æ–°è¼¸å…¥")

if __name__ == "__main__":
    main()
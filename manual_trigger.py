#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®æ­£ç‰ˆæ‰‹å‹•åŸ·è¡Œè…³æœ¬ - è§£æ±ºimportå•é¡Œ
"""

import os
import sys
import csv
import json
import argparse
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict
import logging

# è¨­å®šlogging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SimpleDataManager:
    """ç°¡åŒ–ç‰ˆè³‡æ–™ç®¡ç†å™¨"""
    
    def __init__(self, csv_file: str = "ai_news_data.csv"):
        self.csv_file = csv_file
        self.fieldnames = [
            'hash_id', 'title', 'url', 'summary', 'content', 
            'published_date', 'source', 'category', 'relevance_score',
            'sentiment_score', 'ai_summary', 'collected_date'
        ]
        self._ensure_csv_exists()
    
    def _ensure_csv_exists(self):
        """ç¢ºä¿CSVæª”æ¡ˆå­˜åœ¨"""
        if not os.path.exists(self.csv_file):
            with open(self.csv_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=self.fieldnames)
                writer.writeheader()
            print(f"âœ… å»ºç«‹äº†æ–°çš„è³‡æ–™æª”æ¡ˆ: {self.csv_file}")
    
    def get_statistics(self) -> Dict:
        """å–å¾—è³‡æ–™çµ±è¨ˆ"""
        try:
            if not os.path.exists(self.csv_file):
                return {"error": "è³‡æ–™æª”æ¡ˆä¸å­˜åœ¨"}
            
            df = pd.read_csv(self.csv_file)
            if df.empty:
                return {"error": "æª”æ¡ˆä¸­ç„¡è³‡æ–™"}
            
            df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')
            df['collected_date'] = pd.to_datetime(df['collected_date'], errors='coerce')
            
            # ç§»é™¤æ—¥æœŸè§£æå¤±æ•—çš„è¡Œ
            df = df.dropna(subset=['published_date'])
            
            if df.empty:
                return {"error": "æ²’æœ‰æœ‰æ•ˆçš„æ—¥æœŸè³‡æ–™"}
            
            stats = {
                "ç¸½æ–‡ç« æ•¸": len(df),
                "è³‡æ–™æ”¶é›†æœŸé–“": {
                    "æœ€æ—©": df['published_date'].min().strftime('%Y-%m-%d'),
                    "æœ€æ–°": df['published_date'].max().strftime('%Y-%m-%d')
                },
                "ä¾†æºçµ±è¨ˆ": df['source'].value_counts().head(10).to_dict(),
                "åˆ†é¡çµ±è¨ˆ": df['category'].value_counts().to_dict(),
                "å¹³å‡ç›¸é—œæ€§åˆ†æ•¸": {
                    category: round(group['relevance_score'].mean(), 3)
                    for category, group in df.groupby('category') if len(group) > 0
                },
                "æƒ…æ„Ÿåˆ†æ": {
                    "æ­£é¢æ–‡ç« ": len(df[df['sentiment_score'] > 0.3]),
                    "ä¸­æ€§æ–‡ç« ": len(df[(df['sentiment_score'] >= -0.3) & (df['sentiment_score'] <= 0.3)]),
                    "è² é¢æ–‡ç« ": len(df[df['sentiment_score'] < -0.3])
                }
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"çµ±è¨ˆè³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return {"error": str(e)}
    
    def search_articles(self, 
                       keyword: str = None,
                       source: str = None,
                       category: str = None,
                       date_from: str = None,
                       date_to: str = None,
                       min_relevance: float = None,
                       limit: int = 100) -> List[Dict]:
        """æœå°‹æ–‡ç« """
        try:
            if not os.path.exists(self.csv_file):
                return []
            
            df = pd.read_csv(self.csv_file)
            if df.empty:
                return []
            
            df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')
            df = df.dropna(subset=['published_date'])
            
            # é—œéµå­—æœå°‹
            if keyword:
                mask = (df['title'].str.contains(keyword, case=False, na=False) |
                       df['summary'].str.contains(keyword, case=False, na=False) |
                       df['ai_summary'].str.contains(keyword, case=False, na=False))
                df = df[mask]
            
            # ä¾†æºéæ¿¾
            if source:
                df = df[df['source'].str.contains(source, case=False, na=False)]
            
            # åˆ†é¡éæ¿¾
            if category:
                df = df[df['category'] == category]
            
            # æ—¥æœŸç¯„åœéæ¿¾
            if date_from:
                df = df[df['published_date'] >= pd.to_datetime(date_from)]
            if date_to:
                df = df[df['published_date'] <= pd.to_datetime(date_to)]
            
            # ç›¸é—œæ€§éæ¿¾
            if min_relevance is not None:
                df = df[df['relevance_score'] >= min_relevance]
            
            # æŒ‰ç›¸é—œæ€§æ’åºä¸¦é™åˆ¶æ•¸é‡
            df = df.sort_values('relevance_score', ascending=False).head(limit)
            
            return df.to_dict('records')
            
        except Exception as e:
            logger.error(f"æœå°‹æ–‡ç« æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []

def show_statistics():
    """é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š"""
    data_manager = SimpleDataManager()
    
    print("ğŸ“Š AIæ–°èè³‡æ–™çµ±è¨ˆåˆ†æ")
    print("=" * 50)
    
    stats = data_manager.get_statistics()
    
    if "error" in stats:
        print(f"âŒ {stats['error']}")
        print("\nğŸ’¡ æç¤º:")
        print("1. è«‹å…ˆåŸ·è¡Œæ”¶é›†ç¨‹åºä¾†ç”¢ç”Ÿè³‡æ–™")
        print("2. æˆ–è€…æª¢æŸ¥ ai_news_data.csv æª”æ¡ˆæ˜¯å¦å­˜åœ¨")
        return
    
    print(f"ğŸ“ˆ ç¸½æ–‡ç« æ•¸: {stats['ç¸½æ–‡ç« æ•¸']}")
    print(f"ğŸ“… è³‡æ–™æœŸé–“: {stats['è³‡æ–™æ”¶é›†æœŸé–“']['æœ€æ—©']} ~ {stats['è³‡æ–™æ”¶é›†æœŸé–“']['æœ€æ–°']}")
    
    print(f"\nğŸ“° ä¾†æºçµ±è¨ˆ (å‰10å):")
    for source, count in stats['ä¾†æºçµ±è¨ˆ'].items():
        print(f"   â€¢ {source}: {count} ç¯‡")
    
    print(f"\nğŸ·ï¸  åˆ†é¡çµ±è¨ˆ:")
    for category, count in stats['åˆ†é¡çµ±è¨ˆ'].items():
        avg_score = stats['å¹³å‡ç›¸é—œæ€§åˆ†æ•¸'].get(category, 0)
        print(f"   â€¢ {category}: {count} ç¯‡ (å¹³å‡ç›¸é—œæ€§: {avg_score:.3f})")
    
    print(f"\nğŸ˜Š æƒ…æ„Ÿåˆ†æ:")
    emotion = stats['æƒ…æ„Ÿåˆ†æ']
    total = emotion['æ­£é¢æ–‡ç« '] + emotion['ä¸­æ€§æ–‡ç« '] + emotion['è² é¢æ–‡ç« ']
    if total > 0:
        print(f"   â€¢ æ­£é¢: {emotion['æ­£é¢æ–‡ç« ']} ç¯‡ ({emotion['æ­£é¢æ–‡ç« ']/total*100:.1f}%)")
        print(f"   â€¢ ä¸­æ€§: {emotion['ä¸­æ€§æ–‡ç« ']} ç¯‡ ({emotion['ä¸­æ€§æ–‡ç« ']/total*100:.1f}%)")
        print(f"   â€¢ è² é¢: {emotion['è² é¢æ–‡ç« ']} ç¯‡ ({emotion['è² é¢æ–‡ç« ']/total*100:.1f}%)")

def search_articles_cmd(args):
    """æœå°‹æ–‡ç« å‘½ä»¤"""
    data_manager = SimpleDataManager()
    
    print(f"ğŸ” æœå°‹æ¢ä»¶:")
    if args.keyword:
        print(f"   é—œéµå­—: {args.keyword}")
    if args.source:
        print(f"   ä¾†æº: {args.source}")
    if args.category:
        print(f"   åˆ†é¡: {args.category}")
    if args.date_from:
        print(f"   é–‹å§‹æ—¥æœŸ: {args.date_from}")
    if args.date_to:
        print(f"   çµæŸæ—¥æœŸ: {args.date_to}")
    if args.min_relevance:
        print(f"   æœ€ä½ç›¸é—œæ€§: {args.min_relevance}")
    
    articles = data_manager.search_articles(
        keyword=args.keyword,
        source=args.source,
        category=args.category,
        date_from=args.date_from,
        date_to=args.date_to,
        min_relevance=args.min_relevance,
        limit=args.limit
    )
    
    print(f"\nğŸ“„ æ‰¾åˆ° {len(articles)} ç¯‡æ–‡ç« :")
    print("-" * 80)
    
    if not articles:
        print("æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„æ–‡ç« ")
        return
    
    for i, article in enumerate(articles, 1):
        print(f"{i}. ã€{article.get('category', 'Unknown')}ã€‘{article.get('title', 'No Title')}")
        print(f"   ä¾†æº: {article.get('source', 'Unknown')} | ç›¸é—œæ€§: {article.get('relevance_score', 0):.3f} | æƒ…æ„Ÿ: {article.get('sentiment_score', 0):.3f}")
        print(f"   é€£çµ: {article.get('url', 'No URL')}")
        
        summary = article.get('ai_summary', article.get('summary', 'No Summary'))
        if len(summary) > 100:
            summary = summary[:100] + "..."
        print(f"   æ‘˜è¦: {summary}")
        print()

def create_sample_data():
    """å»ºç«‹ç¯„ä¾‹è³‡æ–™"""
    data_manager = SimpleDataManager()
    
    print("ğŸ“ å»ºç«‹ç¯„ä¾‹è³‡æ–™...")
    
    sample_articles = [
        {
            'hash_id': 'sample1',
            'title': 'OpenAIç™¼å¸ƒChatGPT-5ï¼šçªç ´æ€§AIæ¨¡å‹çš„æ–°é‡Œç¨‹ç¢‘',
            'url': 'https://example.com/chatgpt5',
            'summary': 'OpenAIä»Šæ—¥å®£å¸ƒæ¨å‡ºChatGPT-5ï¼Œé€™æ˜¯è©²å…¬å¸è¿„ä»Šç‚ºæ­¢æœ€å…ˆé€²çš„èªè¨€æ¨¡å‹...',
            'content': '',
            'published_date': datetime.now().isoformat(),
            'source': 'OpenAI Blog',
            'category': 'AI Research',
            'relevance_score': 0.95,
            'sentiment_score': 0.8,
            'ai_summary': 'OpenAIç™¼å¸ƒäº†æ–°ä¸€ä»£ChatGPT-5æ¨¡å‹ï¼Œåœ¨æ¨ç†èƒ½åŠ›å’Œå¤šæ¨¡æ…‹è™•ç†ä¸Šæœ‰é‡å¤§çªç ´ã€‚',
            'collected_date': datetime.now().isoformat()
        },
        {
            'hash_id': 'sample2',
            'title': 'AIè¼”åŠ©è—¥ç‰©ç™¼ç¾ï¼šDeepMindåœ¨è›‹ç™½è³ªçµæ§‹é æ¸¬ä¸Šçš„æ–°é€²å±•',
            'url': 'https://example.com/protein-folding',
            'summary': 'DeepMindçš„AlphaFoldæŠ€è¡“åœ¨è—¥ç‰©ç™¼ç¾é ˜åŸŸå–å¾—æ–°çªç ´...',
            'content': '',
            'published_date': (datetime.now() - timedelta(days=1)).isoformat(),
            'source': 'DeepMind Blog',
            'category': 'Biotech',
            'relevance_score': 0.88,
            'sentiment_score': 0.6,
            'ai_summary': 'DeepMindåˆ©ç”¨AIæŠ€è¡“åœ¨è›‹ç™½è³ªçµæ§‹é æ¸¬ä¸Šå–å¾—çªç ´ï¼Œæœ‰åŠ©æ–¼åŠ é€Ÿæ–°è—¥é–‹ç™¼ã€‚',
            'collected_date': datetime.now().isoformat()
        },
        {
            'hash_id': 'sample3',
            'title': 'Teslaæ©Ÿå™¨äººOptimusé–‹å§‹å•†æ¥­åŒ–éƒ¨ç½²',
            'url': 'https://example.com/tesla-robot',
            'summary': 'Teslaå®£å¸ƒå…¶äººå½¢æ©Ÿå™¨äººOptimuså°‡é–‹å§‹åœ¨å·¥å» ç’°å¢ƒä¸­é€²è¡Œå•†æ¥­åŒ–æ¸¬è©¦...',
            'content': '',
            'published_date': (datetime.now() - timedelta(days=2)).isoformat(),
            'source': 'TechCrunch AI',
            'category': 'Robotics',
            'relevance_score': 0.82,
            'sentiment_score': 0.4,
            'ai_summary': 'Teslaçš„äººå½¢æ©Ÿå™¨äººOptimusé–‹å§‹å•†æ¥­åŒ–æ¸¬è©¦ï¼Œæ¨™èªŒè‘—æ©Ÿå™¨äººæŠ€è¡“çš„é‡è¦é€²å±•ã€‚',
            'collected_date': datetime.now().isoformat()
        }
    ]
    
    # å¯«å…¥CSVæª”æ¡ˆ
    with open(data_manager.csv_file, 'a', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=data_manager.fieldnames)
        for article in sample_articles:
            writer.writerow(article)
    
    print(f"âœ… å·²å»ºç«‹ {len(sample_articles)} ç­†ç¯„ä¾‹è³‡æ–™")

def main():
    parser = argparse.ArgumentParser(description='AIæ–°èæ”¶é›†å™¨ - è³‡æ–™åˆ†æå·¥å…·')
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # çµ±è¨ˆå‘½ä»¤
    stats_parser = subparsers.add_parser('stats', help='é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š')
    
    # æœå°‹å‘½ä»¤
    search_parser = subparsers.add_parser('search', help='æœå°‹æ–‡ç« ')
    search_parser.add_argument('--keyword', help='é—œéµå­—')
    search_parser.add_argument('--source', help='ä¾†æº')
    search_parser.add_argument('--category', help='åˆ†é¡')
    search_parser.add_argument('--date-from', help='é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)')
    search_parser.add_argument('--date-to', help='çµæŸæ—¥æœŸ (YYYY-MM-DD)')
    search_parser.add_argument('--min-relevance', type=float, help='æœ€ä½ç›¸é—œæ€§åˆ†æ•¸')
    search_parser.add_argument('--limit', type=int, default=20, help='æœ€å¤§çµæœæ•¸')
    
    # å»ºç«‹ç¯„ä¾‹è³‡æ–™å‘½ä»¤
    sample_parser = subparsers.add_parser('sample', help='å»ºç«‹ç¯„ä¾‹è³‡æ–™')
    
    args = parser.parse_args()
    
    if args.command == 'stats':
        show_statistics()
    elif args.command == 'search':
        search_articles_cmd(args)
    elif args.command == 'sample':
        create_sample_data()
    else:
        parser.print_help()
        print("\nğŸ’¡ ä½¿ç”¨ç¯„ä¾‹:")
        print("python manual_trigger.py sample    # å»ºç«‹ç¯„ä¾‹è³‡æ–™")
        print("python manual_trigger.py stats     # æŸ¥çœ‹çµ±è¨ˆ")
        print("python manual_trigger.py search --keyword ChatGPT  # æœå°‹æ–‡ç« ")

if __name__ == "__main__":
    main()